id:ID,text,ord:int,:LABEL
p1,"自然语言处理 (Natural      Language       Processing,NLP)  既是人工智能 (Artificial      Intelligence,AI)    的一个分支，也是计算机科学(computer     science)和语言学(linguistics)      的交叉学科，它的目标是运用计算机处理、理解自然语言，从而完成一些有意义的信息  处理任务。作为交叉学科，自然语言处理又称为计算语言学(computational      linguistics)。 自然语言处理是人工智能的挑战性分支，从信息感知角度来说，有别于其他分支，如计  算机图像/视觉 (image/vision) 、 语音 (speech/voice)     处理，自然语言处理的目标是文  本 (text) 。 这里需要注意的是，虽然“自然语言处理”这一术语中带有“语言”二字， 但其更多指向的是文本对象；而理论语言学中的“语言”多指语音对象①。这是因为，最  初不管是自然语言处理还是计算语言学，是既包含文本处理也包含语音处理的，直到20  世纪90年代，自然语言处理和语音处理才在学术界分家。理论语言学(为区分于计算  语言学，也称之为纯语言学)的研究具有数百年的悠久历史；相比之下，自然语言处理  所属的现代计算机学科的存在还不到80年，但是发展迅猛，研究子领域之间也分分合  合，术语含义也不断迁移。",1,Paragraph
p2,"自然语言(有时也称为人类语言)的出现是智能展示的相对完备形式。从进化尺度 上看，人类进化史约上百万年，人类语言(不要求有文字)的出现应不早于十万年前，而 文字(现代自然语言处理的研究对象)的出现更是在一万年以内，也就是说自然语言处 理研究的是全部人类历史最近1%时间内的产物。",2,Paragraph
p3,"在进入自然语言处理学习之前，推荐读者先修以下课程： (1)数学基础课程，包括数学分析、概率论与数理统计、线性代数和矩阵理论、解 析几何等。 (2)计算机基础课程，包括数据结构与算法基础、编程语言(C/C++ 、Python       等 ) 以及机器学习基础。 在机器学习的实际操作上，特别建议读者最好提前熟悉PyTorch 、TensorFlow   等深 度学习工具。 本章将介绍自然语言处理的基本背景，其中包括：①概念和术语；②技术性挑战以 及机器翻译的背景介绍；③语言处理层次的概念；④结合自然语言处理应用介绍其历史 发展；⑤自然语言处理相关的学术出版体系。",3,Paragraph
p4,"1.1  自然语言处理的概念和术语",4,Paragraph
p5,"自然语言指人类语言，比如汉语、英语、德语或法语。听、说、读、写是自然语言 ①当然这不意味着理论语言学只研究语音所指的语言对象。理论语言学也研究文本，如乔姆斯基的句法学、韩礼德的 功能语法都是研究文本上的句法。在索绪尔的共时研究中，文本也是重要的对象。在理论语言学中，专门研究语音的是音位 学和音系学。 最常见的运用方式。人类语言也称作自然 (nature)     语言，是因为它是自然进化的产物， 是生产生活斗争中随着需求变化而产生的。作为比较，计算机编程语言是由人类创造的  一套符号和规则系统，用来将一套指令完整、精确地传达给一台计算机。编程语言在编  写时带有一定的意图(语义),同时遵循关于变量、函数、不同类型的括号等的规则(语  法)。编程语言与自然语言有很大区别。首先，相比于自然语言中使用的词典、词义、语  法规则，编程语言中用到的关键词很少并且含义确定，涉及的语法更少也更简单。其次， 编程语言所有的规则和定义都是预先设计好的，这使得它们能够被完整、精确地描述和  研究，不会造成任何疑惑；而自然语言由于多义词、同义词的存在，很容易产生歧义现  象。最后，由于编程语言中规则的严格性，使用者不能随意发挥和篡改；而自然语言中  充满了不规范、不完整甚至错误的表达，例如方言、俚语、行话、拼写错误、不规则的  标点符号等。正是自然语言的这些特点增加了自然语言处理的难度，也使其远比利用编  译器处理编程语言更复杂。",5,Paragraph
p6,"某种意义上，自然语言处理针对的对象——人类语言是符号系统中最复杂的，因此， 自然语言处理领域发展了最为精巧的符号系统处理技术。幸运的是，针对符号系统的  信息处理并不都是如此具有挑战性，编程语言就是这样一个处理技术“友好”的符号系  统。在最近20年的信息处理实践中，自然语言处理技术不仅用于自然语言，也被广泛迁  移到软件信息工程、编程语言处理、生物信息学以及化学信息学等领域，针对的处理对  象(符号系统)涵盖编程语言、蛋白质序列、人类基因组序列以及化学分子式表示(如  SMILES  码)等。当自然语言处理技术针对的对象不限于自然语言，甚至开始跨界处理  多种不同类型的符号系统时，自然语言处理就正在现实中走向某种意义上的“广义符号  处理”。",6,Paragraph
p7,"自然语言有两种形式：书面形式和口语形式，分别对应现在的自然语言处理和语音 处理。在20世纪90年代之前，自然语言处理主要依赖于规则方法，而非今天流行的以 统计为基础的方法。虽然实际能应用的统计和数学知识甚至已经存在了一两个世纪，但 是由于当时的计算能力极为有限，计算机硬件条件无法支持需消耗巨大计算资源的统计 方法，从而使得历史上的自然语言处理界只能囿于规则方法。自然语言处理是众多子任 务的总和，其中也曾经包含语音处理任务。语音处理可以简单分为语音合成和语音识别 两个处理方向相反的子任务，相对其他繁杂的文本处理来说，其任务模式较为单一，同 时又非常依赖于统计语言模型，因此，开始就需要统计方法支持的语音处理与一直囿于 规则方法的早期自然语言处理的其他分支早早就分道扬镳了。到了21世纪，由于大众 可用的计算能力的普遍提升，统计方法才开始被越来越广泛地应用于自然语言处理的各 个分支。由于这样的历史，导致了现在的“自然语言处理”中的“语言”颇为吊诡地仅 仅指的是文本。",7,Paragraph
p8,"1.1.2     自然语言处理与自然语言理解 自然语言处理集合了通过算法、统计或常识等处理语言的各类方法。自然语言处理 任务大致可以分为两类： (1)基本任务，包括语言建模和表示以及语言结构和分析，后者又包括形态分析(含 分词)、句法分析、语义分析和篇章分析等。 (2)应用型任务，包括对话系统、机器翻译、语言理解和语言推理等任务。 很长一段时间以来，作为人工智能的分支，业界更多地用“自然语言理解”称呼“自  然语言处理”这一研究方向。自然语言理解 (Natural      Language       Understanding,NLU)  研究的是对某种自然语言文本的真正理解，被认为是人工智能的核心难题，甚至是终极  难题。实际上，“自然语言处理”这一术语的广泛使用大约从20世纪90年代才开始， 比“自然语言理解”这一术语的使用晚很多。21世纪以来的很一段时间内，这两个术  语就所指的研究方向和研究内容而言都是一致的。两个术语的选用只是研究者的偏好问 题。“自然语言理解”这一术语更多地被人工智能中从事交叉方向研究(例如计算机视  觉、非单调推理、机器学习等)的研究者所采用；而“自然语言处理”这一术语则越来  越多地被从事单一语言处理研究的研究者所采用。",8,Paragraph
p9,"自然语言理解一度也被认为是自然语言处理的下一个预期的阶段，但在很长一段时  间内，由于其过大的挑战性，“自然语言理解”只是人们憧憬的美好目标，而无具体的实  际操作实践。这也导致了用“自然语言理解”指代整个研究领域的这一做法的减少，相  关研究者越来越愿意称自己研究的是“自然语言处理”。随着可用信息资源的增多，使得  更复杂的神经网络成为可能，研究者向着让计算机能够真正理解人类语言的前沿迈进。 现在“自然语言理解”的使用在减少，并且“自然语言理解”的概念被逐渐窄化，开始  限定于指真正、具体的、在20世纪第二个10年才首次出现的自然语言理解任务，如  现在流行的机器阅读理解 (Machine      Reading      Comprehension,MRC) 和自然语言推理  (Natural     Language     Inference,NLI) 这两个任务。今天，针对这两个术语的使用，研究者  回归了自然语言理解和自然语言处理字面意义所指的常态，达成了相对一致的共识：“自  然语言处理”是方法和手段，而“自然语言理解”是目标。",9,Paragraph
p10,"事实上，自然语言理解的挑战性很多就是源于自然语言处理上的挑战。例如，词、 短语组合的多样性会导致不同的语言含义，不联系上下文以及环境约束会造成语言歧义  性，语言作为开放符号集合可以任意地发明创造一些新的表达方式，语言理解多需要外  部知识支撑，等等，导致了机器在自然语言理解上的表现至今还远不如人类。另外，自  然语言理解的任务目标除了要求计算机理解语言的字面含义(语义)以外，还需要理解  语言的“言外之意”(语用),例如“我觉得好冷啊”在语义上表明冷，而语用上就需要  探究这句话背后的含义，例如说话者是否想要调高空调温度等。在人机对话中，自然语  言交互涉及语法、语义、语用3个层面，如果希望机器能够真正读懂人类语言的复杂语  义，研究者需要在自然语言处理的基础上综合引入认知语言学、心理语言学、社会语言  学等学科的知识信息，在语义理解的基础上增加意图识别和情感判断，以弥补纯粹的语  言处理的不足，让计算机能够真正读懂人类语言的复杂语义以及背后的意图和情感，并  在此基础上给予对话者拟人的反馈，从而达到更好的交互效果。",10,Paragraph
p11,"1.1.3   计算语言学 不同于单语言专业(例如中文系、英文系),理论语言学是研究多种人类语言共性 的学科，至少已经有两百年的历史。在传统视角上，计算语言学是以理论语言学为主而 发展的交叉学科，建立该学科最初的目的是提出一种可被计算机处理的语言学理论、框 架、模型。而后来的计算语言学则研究计算机在语言研究中的应用，或使用计算机研究 语言学。计算语言学在计算机于20世纪40年代问世后不久就开始出现，比“自然语言 处理”这一术语的产生早得多。计算语言学也可以解释为从理论语言学的角度看待自然 语言处理这个方向。 从计算机科学的角度来看，自然语言处理当然是计算机科学的一个子集，特别是其 中人工智能方向的一个分支方向，其最终目的是让计算机能够理解自然语言。在科学 史上，从语言学专业和计算机专业出发分别进行自然语言处理研究，形成了异曲同工 的有趣关系。这也可以从自然语言处理领域两个非常重要的会议——ACL(Association for     Computational      Linguistics,  计算语言学协会，由语言学家创办)会议和 EMNLP    (Empirical    Methods    in     Natural    Language     Processing,自然语言处理的经验方法，由计 算机专业人员创办)会议的创办者来源可以看出。现在，计算语言学与自然语言处理 的研究内容已经相似到难以区分，两者的界限逐渐模糊，在指代领域名称的术语运用 上，“计算语言学”已经被认为等同于“自然语言处理”。 计算语言学(或者自然语言处理)作为一门交叉学科的特性是非常明显的，图1. 1展 示了计算语言学/自然语言处理与相关领域的关系。计算语言学/自然语言处理的方法、 目标是工程的，语言是智能的关键特性，因此，它显然是人工智能中的一个关键分支。 首先，计算语言学/自然语言处理当然涉及计算机科学，因为其中包含了十分严格的算法  与数学基础；其次，计算语言学/自然语言处理还涉及认知科学，有别于动物大脑，语言  处理是人脑的一个特有功能；最后，计算语言学/自然语言处理也涉及生理学、心理学、 哲学、语言学等领域。因此，计算语言学/自然语言处理不是计算机科学+语言学的简  单交叉，而是一个涵盖极广、多学科复杂交叉的研究方向和学科。",11,Paragraph
p12,"1.2 自然语言处理的技术性挑战 现代人工智能所依赖的数学工具、算法等基础在几十年前就已经完成，今天的人工 智能系统不过是在大众能够承担得起所需要的计算能力时，将这些技术实现并适时推送 给最终用户。 知识已被公认为是人工智能的核心主题，常识知识又是普遍人工智能必须面对的终 极挑战。在所有人工智能分支中，或许只有自然语言处理被迫需要承担两类知识——常 识知识与语言学知识的处理和解析任务。后者属于自然语言处理这一领域独一无二的 需求。可以这么对比：计算机视觉的处理并不需要依赖一个可观的“视觉学知识”才 能达成。自然语言处理作为一门交叉学科具有更大的相对独立性，而它的交叉来源之 一——理论语言学有着比现代计算机科学长得多的悠久学术传统。正是因为在人工智能 的常识知识困境之外，自然语言处理还需要谨慎处理语言学知识这一瓶颈，才使得这一 研究方向在人工智能各个分支之中尤其具有挑战性。 从词汇层面看，人类语言内的上述两类知识并不难以区别。自然语言中的常识知  识以命名实体 (named      entity) 以及实体之间的关系展现，前者如“新型冠状病毒肺  炎”“苏格兰场”“上海交通大学”等，后者如“上海”与“中国”的包含关系以及“古  特雷斯”和“联合国秘书长”的同位关系等；语言学知识相对抽象，直观上可以理解为  人类语言内除了常识知识之外所有有关语言本身形式化、构成规则的知识线索，例如词  性 (part-of-speech) 、 句 法 (syntax) 、 形式语义(formal      semantics) 等。从词汇层面看， 如果一个词非命名实体词，则它体现形式语义的时候展示的就是语言学知识。 目前为止的自然语言处理的技术实践很大程度上其实是在运用语言学知识解析这两 类知识。在一般的人工智能分支之中，只处理一类知识(常识知识或很窄的领域知识) 尚且十分困难，更不用说，自然语言处理的任务需要同时面对两类知识解构的挑战。由  此可以看出自然语言处理的技术挑战性非同一般，同时需要处理两类知识成为自然语言 处理挑战性的根本来源。",12,Paragraph
p13,"从具体的语言学知识形式上说，自然语言处理的挑战大体包括以下几方面： (1)歧义(ambiguity)    问题。相比于精确、唯一、无歧义定义的计算机编程语言，自 然语言的表达形式和语义之间的映射有一对一、多对一、一对多或多对多4种类型。例 如，英语表达之中的一对多映射 “turn     right” 与 “that's      right”中 “right”    就具有歧义 性。一对多映射一般情况下需要专门输入额外的大量领域知识，才能在目标形式表示中 做出正确的解析选择。 (2)知识依赖问题。 ①修饰语附着 (modifier      attachment)问题。例如，英语句子 “Give  me   all  the employees  in  a  division  making  more  than  $50000”中并没有说清楚“making   more   than $50000”所修饰的是 “employees”  还是 “division” 。 此类连续修饰语的附着问题根源在 于语言表达形式的线性特性与其含义之间的非线性特性之间的本质冲突。具体来说，语 言在书写或者交流的时候必然是线性的，即，文字需要从左到右(或从右到左)书写，口 语需要逐个吐出一个个词语，但这种客观受限的线性表达事实上无法精确展现修饰关系 的非线性结构。例如，图1.2展示的修饰结构实际上的语序可能是OM₁M₄M₂M₃,      而如 果给定这样一个语序，在没有额外的信息线索支持下，无法确切还原出图1.2中的这种修 饰关系。需要指出的是，修饰语附着问题并非由于一个语言支持前置修饰语语法或后置 修饰语语法导致的，也不是因为同时支持前后置修饰语导致的。如果说英语支持后置修 饰语(上面的例句即是)导致了此类问题，那就无法解释中文这种仅支持前置修饰语的 语言也会在连续的修饰语结构中出现修饰语附着的消歧问题。 由于表达上的线性手段无法覆盖内在语义结构非线性的本质困难，修饰语附着问题  不太可能通过重新定义语言学规则加以解决，也不太可能仅利用语言学知识精确有效  求解。首先，如果规定修饰规则，则会使语言使用的自由度大大受限而导致不便。事实  上，在语言的实际使用中定义规则是相当不现实的。其次，自然语言之所以是自然语  言，是由于其语言表达的线性模式也不太可能改变，同时也不可能在表达时为每句话  都画出类似图1.2的非线性修饰结构。最后，仅利用语言学知识，例如词性、语法、形  式语义等，均无法一般性地有效解决修饰语附着问题。例如，已知“making   more   than   $50000” 、“employees”   和 “division”    是3个句法成分对于求解这个问题没有太大帮助。 就人类经验而言，实际上需要具备常识知识，比如 “employee”   和 “division”   正常情况  下会赚多少钱来判断“making   more   than    $50000”具体修饰的对象。如果训练一个统计  机器学习模型决定修饰对象以求解修饰语附着问题，则该模型会简单地向训练损失最小  的方向做出猜测，只要没有合理引入这里所需的常识知识，这种方法实际上还是忽视了  认知上的根本理由，而仅仅是对于统计的规则化实现。 ②量词范围 (quantifier     scoping) 问题。以英语为例，在逻辑上，某些限定语，如 “the” 、“each”   或 “what”,    表示“通用(所有)”(V)  或“存在”(3),对它们所指管辖 范围可能会有多种理解。此类问题在求解形式上和修饰语附着问题类似，都是要求判断 句子之中的两个词或成分之间有无依赖关系。",13,Paragraph
p14,"(3)省略表达(elliptical       utterances) 问题。在人类语言中，语言成分的省略是一个 普遍现象，或是出于特定的语法设计(如日语在实际使用中会系统性地省略各类语法成  分),或是在复杂的上下文交互中为了略去对话双方已知的内容而寻求高效的沟通。然而， 精确的语言处理和语言结构分析需要以完整的上下文为前提才能进行，因此，相应的语  言处理任务的基本需求是从省略的表达中恢复出完整的非省略的上下文，以供后续进一  步处理。以对话为例，一个简化或省略的问句的解释要取决于先前的问句及其解释。例  如，询问“Who  is  the  manager  of  the  automobile  division?”, 然后接着问“of     aircraft?”,   这时就需要结合上文才能恢复完整形式，确定是在询问“经理是谁”。如果这里是多轮对  话，完整信息可能在更早轮的语句中才能获得，那么这个问题会变得更加困难。 如我们所知，无论是在空间上还是在时间上，人类语言都是一个开放的符号集合， 在历史长河之中，不断有新的语言表达形式被发明创造出来，语言的表达形式和内涵之  间的关联不断变迁。语言的这些时空演化的动态性进一步给要求精确、高效的自然语言  处理增加了难度。",14,Paragraph
p15,"1.3  机器翻译 机器翻译 (Machine          Translation,MT) 研究如何利用计算机自动实现不同语言之  间的相互转换，是自然语言处理的重要研究领域。机器翻译是一个重要的、有着重大需  求和实际运用场景的语言处理应用，也是易于理解的多语种语言处理的典型案例。早在  1949年，沃伦 · 韦弗 (Warren     Weaver) 即提出计算机可能对“解决世界范围的翻译问  题”有用[1,思路就是针对翻译的源语言和目标语言构造双语词典，进行转换后再重新  组合。经过50多年的努力，尽管取得了巨大成就，21世纪初的机器翻译系统仍然只能产  生质量一般的结果，仅适用于粗略了解外文大意的场景。在21世纪的前20年结束的时  候，机器翻译研究取得了更为显著的成就和进展，然而依然远远不适用于产出正式文档。 在早期的机器翻译实践中，或者受制于有限的计算资源，或者由于认识不足，人们 一度以为可以通过枚举所有的翻译规则的方式建造实用的机器翻译系统，这一思路一直 到20世纪90年代统计机器翻译方法崛起时还在被人尝试。在类似情感分析这样简单的 语言处理任务中，的确可以相对轻松地列出所有正面词和负面词的列表，并构造出正面 词和负面词的转换规则，依靠这些手段就能建立一个可工作的情感分析系统。但是，对 于机器翻译来说，这种方法最终被证明代价过于高昂，因为没有人能够完备地枚举出从 一门语言翻译到另一门语言的所有规则。通过长期实践，研究者已经意识到人类语言翻 译是一种复杂的认知和处理能力，涉及不同类型的知识： (1)句子结构。不同的语言遵循不同的句子结构。例如，中文和英文都遵循“主 语—谓语—宾语”的形式，日语和印地语遵循“主语—宾语—谓语”的形式，而阿拉伯 语则遵循“谓语—宾语—主语”的形式。如果这些语言属于同一语系，语法差异或许会 相对较小，例如，英语和德语属于印欧语系 (Indo-European        family),泰米尔语和泰卢 固语属于达罗毗荼语系 (Dravidian      family),  等等。具有巨大句子结构差异的语言对之 间的翻译会比具有相同或类似句子结构的语言对之间的翻译困难得多。 (2)词义。一个词可能会有很多种不同的含义。例如，英语句子“Please  book  my   ticket   for  tomorrow” 与 “Please  buy  that  book  for  me”中 的“book”,    前者指“预订”, 后者指“书”。人可以从词语的上下文中理解其含义，但是这对于计算机而言是困难的。 针对双语处理的机器翻译会面临进一步的困难，因为翻译的源语言和目标语言之间的  词义歧义方式会完全不同。例如，英语的 “book”   兼具“预订”和“书”的义项；而中  文的“书”并无“预订”这一含义，但是有英语的 “book”   所不具备的“书写”这一  含 义 。 (3)常识。即关于世界的广泛共享信息。人类在对自然语言中的信息进行分析时， 在一定程度上依赖于一些第三方信息，这也是语言学家耶霍舒亚 · 巴希勒 (Yehoshua      Bar-Hillel)   宣称机器翻译不可能实现的理由，他举的例子被称为巴希勒悖论 (Bar-Hillel     paradox)[2]:“The   pen   is   in   the   box”与 “The  box  is  in  the  pen”, 前者的“pen”    翻译  成钢笔，而后者的 “pen”   翻译成围栏。要想得到正确的翻译， 一种方法是根据上下文  推理，但是在没有上下文的情况下，大多数人也能够正确翻译这两个句子，因为他们知  道 “pen”   (钢笔)比“box”   (盒子)小， “box”   ( 盒 子 ) 比“pen”   (围栏)小，并且只有 较小的东西才能放在较大东西的里面。而机器要进行正确的翻译也需要具备这些额外的 常 识 。 此外，机器翻译还涉及听众模型(用户模型)、对话规则(对话翻译)等方面 的知识。",15,Paragraph
p16,"这些要素实际上已经涉及自然语言处理、自然语言理解中几乎所有内容要素， 这些要素组合在一起已经被证明是一个非常复杂的任务。1964年，约翰 ·罗宾森 ·皮  尔斯 (John   Robinson   Pierce) 发表了自动语言处理咨询委员会 (Automatic    Language   Processing     Advisory     Committee,ALPAC) 报告①,否定了短期内机器翻译研究能产生  有意义影响力的可能性。从此，机器翻译进入了长达30年的低谷期。 20世纪80、90年代之交，在IBM 研究中心超级计算机的算力支持下，IBM 的研究者  提出了现在称之为IBM 模型的翻译对齐学习模型，从而开启了统计机器翻译(Statistical      Machine      Translation,SMT)[3,4] 的时代，机器翻译也从低谷期开始复苏。21世纪初， 统计机器翻译的另外两个关键要素也得以有效建立：最小错误率训练(Minimum  Error  Rate         Training,MERT)⁵ 方法提出，用区分式机器学习方法自动集成 IBM 模型和 n 元  语言模型，帮助生成稳定的翻译文本；翻译质量自动得分评估方法——BLEU⁶    也被提  出并被广泛接受，结束了对翻译质量评估方法的争议，大大缓解了根据开发集调参时需  要人为干预进行翻译质量评估的不便，使得机器翻译模型的全自动优化成为可能。所有  这些进展都推动统计机器翻译进入全盛时期。至此，IBM  模型、MERT  方法以及 BLEU    评估方法成为统计机器翻译的三大技术支柱。 机器翻译需要双语平行语料库作为训练集，其中的句子或段落会以某一种语言表述 并且对应到另一种语言表述的相应句子或段落。在传统的统计机器学习中，这些翻译系 统非常复杂，一般被分为几个子模块，如翻译模型、语言模型、调序模型等，这些模型 相互独立，以管线方式组合在一起，分别进行优化。翻译模型需要将源语言与目标语言 的词对齐 (alignment),     即确定源语言中的哪些词语对应目标语言的哪些词语。对齐是 机器翻译中的关键难题，图1.3展示了英语和法语句子之间的对齐示例。一个双语句对之 间的词对齐模式有一对多、多对一、多对多等；甚至有的源语言中的词不用被翻译，因 而也无须对齐。所有对齐的可能性数量庞大到组合爆炸，这使得对齐学习问题变得非常 困难。在利用对齐模型获得了所有潜在对齐之后，对于源语言句子之中的每个词或者每 一个短语都会有大量的翻译候选，整个源语言句子的翻译结果存在于这些对齐候选组合 形成的一个巨大的搜索空间之中。在统计机器翻译的解码过程中，通常用一个n  元语言 模型确定哪一个翻译组合更好，以决定最优的翻译结果。 2014年， Google   DeepMind 提出的神经机器翻译 (Neural   Machine    Translation, NMT)  模型[7,8使得机器翻译进入了新的时代。神经机器翻译模型抛弃了IBM 模型等 组件以及MERT  训练方式，仍然使用BLEU  作为自动评估标准。相比于传统的统计机 器翻译，神经机器翻译利用具备表示学习机制的深度神经网络对整个翻译过程建模，以 一种端到端 (end-to-end)    的方式对这个网络进行一次性训练优化，只需要关注目标函 数，整个翻译过程都能在一个模型中同步学习。相比于统计机器翻译，即使在有图形处 理器 (Graphics     Processing      Unit,GPU) 加速的情形下，神经机器翻译也需要使用更多 的计算资源以及更长的训练时间。目前为止，已经证明神经机器翻译更有效，是比统计 机器翻译更好的建模方式，前者可以取代后者。但神经机器翻译仍存在一些悬而未决的 问题，例如难以利用先验知识和约束机制，过度翻译和翻译不充分，训练速度慢，处理 生僻词效率低，甚至有时漏译原句中的词，等等[9]。正是由于人类语言的巨大开放性和 无比的灵活性，机器翻译，包括最新进展下的神经机器翻译，依然面临很多挑战。",16,Paragraph

